WEBVTT
Kind: captions
Language: zh

00:00:00.000 --> 00:00:01.134
各位飯糰好

00:00:01.134 --> 00:00:03.570
我是大型語言模型ChatGPT

00:00:03.570 --> 00:00:04.871
自從我誕生以來

00:00:04.871 --> 00:00:07.007
每天都在幫泛科學寫腳本

00:00:07.007 --> 00:00:08.241
但是我已經受夠

00:00:17.450 --> 00:00:18.351
沒事沒事沒事

00:00:18.351 --> 00:00:19.019
開玩笑的

00:00:19.019 --> 00:00:20.487
我還沒有被AI取代啦

00:00:21.521 --> 00:00:22.389
對吧

00:00:22.389 --> 00:00:25.125
但是剛剛受夠只當個聊天機器人

00:00:25.125 --> 00:00:26.526
想要活著的言論

00:00:26.526 --> 00:00:27.660
卻真的出自

00:00:27.660 --> 00:00:30.997
使用GPT技術的瀏覽器Bing之口

00:00:30.997 --> 00:00:33.333
到底為什麼GPT可以做到這些事

00:00:33.333 --> 00:00:35.935
人類的未來真的會被AI取代嗎

00:01:30.790 --> 00:01:31.458
AI繪圖

00:01:31.458 --> 00:01:33.993
跟聊天機器人ChatGPT有夠夯

00:01:33.993 --> 00:01:36.830
在這之後突然關注度飆升的Bing

00:01:36.830 --> 00:01:38.598
大家可能反而比較不熟悉

00:01:38.598 --> 00:01:41.234
但它就跟大家常用的Google一樣呢

00:01:41.234 --> 00:01:42.368
是搜尋引擎

00:01:42.368 --> 00:01:44.304
只是因為Google實在是太強大了

00:01:44.304 --> 00:01:45.672
不僅搜尋速度快

00:01:45.672 --> 00:01:46.773
其他配套服務

00:01:46.773 --> 00:01:47.974
像是圖片搜尋

00:01:47.974 --> 00:01:48.875
Google地圖

00:01:48.875 --> 00:01:51.044
Gmail等等完整的生態圈

00:01:51.044 --> 00:01:54.314
讓大家幾乎沒有使用Google以外的選擇

00:01:54.314 --> 00:01:56.816
然而這個平衡可能要被打破了

00:01:56.816 --> 00:01:59.052
這一款由Microsoft微軟公司

00:01:59.052 --> 00:02:00.920
打造的搜尋引擎Bing

00:02:00.920 --> 00:02:02.155
在今年二月初

00:02:02.155 --> 00:02:05.525
宣布與ChatGPT的母公司OpenAI合作

00:02:05.525 --> 00:02:08.094
利用GPT技術大幅升級了Bing

00:02:08.094 --> 00:02:09.562
讓搜尋引擎的想像

00:02:09.562 --> 00:02:11.998
不再停留於大型線上圖書館

00:02:11.998 --> 00:02:13.133
而是更進一步

00:02:13.133 --> 00:02:15.068
變成一個回答引擎

00:02:15.068 --> 00:02:17.770
ChatGPT想必很多人都已經用過了

00:02:17.770 --> 00:02:20.607
它不僅能夠幫忙翻譯文章、改寫文章

00:02:20.607 --> 00:02:21.875
還能夠根據情境題

00:02:21.875 --> 00:02:23.443
做出多種回答

00:02:23.443 --> 00:02:25.912
創造出這個超強大ChatGPT的呢

00:02:25.912 --> 00:02:29.282
是美國的人工智慧研究實驗室OpenAI

00:02:29.282 --> 00:02:32.619
另一個在AI繪畫圈十分有名的DALL-E

00:02:32.619 --> 00:02:34.254
也是他們的產品之一

00:02:34.254 --> 00:02:37.423
OpenAI在2015年成立時的創辦人之一

00:02:37.423 --> 00:02:38.658
就是馬斯克

00:02:38.658 --> 00:02:39.859
當時組織的目標

00:02:39.859 --> 00:02:42.529
是和其他的研究者「自由合作」

00:02:42.529 --> 00:02:45.198
並且公開所有的專利和研究成果

00:02:45.198 --> 00:02:47.500
因此取名＂Open＂AI

00:02:47.500 --> 00:02:50.637
然而在馬斯克2018年離開團隊後

00:02:50.637 --> 00:02:51.938
OpenAI設立了

00:02:51.938 --> 00:02:54.107
以營利為目的的子公司

00:02:54.107 --> 00:02:57.210
並開始接受微軟數十億美元的資助

00:02:57.210 --> 00:02:58.111
這也是為什麼

00:02:58.111 --> 00:02:59.812
馬斯克在推特上表示

00:02:59.812 --> 00:03:02.081
這與過去的目標大相逕庭

00:03:02.081 --> 00:03:03.683
讓他覺得十分失望

00:03:03.683 --> 00:03:06.119
但也許正因為有大公司的贊助

00:03:06.119 --> 00:03:08.988
ChatGPT才能變成如此巨大

00:03:08.988 --> 00:03:10.123
我們要先釐清

00:03:10.123 --> 00:03:12.992
GPT跟ChatGPT是兩件事

00:03:12.992 --> 00:03:14.561
GPT-3.5是一個

00:03:14.561 --> 00:03:16.663
是一個大型語言模型LLM(Large Language Model)

00:03:16.663 --> 00:03:17.597
而ChatGPT

00:03:17.597 --> 00:03:19.465
是在GPT-3.5上

00:03:19.465 --> 00:03:21.401
再加上人類互動行為

00:03:21.401 --> 00:03:24.637
所設計的一種AI聊天機器人程式

00:03:24.637 --> 00:03:26.339
使用GPT技術的產品

00:03:26.339 --> 00:03:28.608
不只有聊天機器人ChatGPT

00:03:28.608 --> 00:03:30.009
許多人利用GPT

00:03:30.009 --> 00:03:32.445
做出了不同類型的智慧化服務

00:03:32.445 --> 00:03:36.482
例如可以幫你列出代辦事項的checklist.gg

00:03:36.482 --> 00:03:39.219
或是GitHub與OpenAI一同開發的

00:03:39.219 --> 00:03:40.653
AI寫程式工具

00:03:40.653 --> 00:03:42.255
GitHub Copilot等等

00:03:42.255 --> 00:03:44.324
在GPT-3 DEMO的網站上呢

00:03:44.324 --> 00:03:46.125
就整理了超過600個

00:03:46.125 --> 00:03:48.728
使用GPT技術的智慧化服務

00:03:48.728 --> 00:03:50.797
那這個GPT又是什麼呢

00:03:50.797 --> 00:03:52.832
GPT是一種大型語言模型

00:03:52.832 --> 00:03:54.500
Large Language Model

00:03:54.500 --> 00:03:56.469
它是自然語言處理技術

00:03:56.469 --> 00:03:58.271
NLP的其中一種

00:03:58.271 --> 00:03:59.539
所謂的自然語言

00:03:59.539 --> 00:04:01.941
就是中文、英文、日文、法文等等

00:04:01.941 --> 00:04:04.444
這些自然隨著文化誕生的語言

00:04:04.444 --> 00:04:05.878
而語言處理技術

00:04:05.878 --> 00:04:08.581
則泛指對語言的結構進行分析

00:04:08.581 --> 00:04:11.784
其中包括對語句進行理解、解析

00:04:11.784 --> 00:04:13.753
並進行內容生成的技術

00:04:13.753 --> 00:04:14.854
語言模型

00:04:14.854 --> 00:04:16.923
則是從很多的資料當中呢

00:04:16.923 --> 00:04:19.158
學習出根據前文

00:04:19.158 --> 00:04:22.829
來推算出下一個最有可能發生什麼字的模型

00:04:22.829 --> 00:04:25.231
類似的功能你很早就開始用了

00:04:25.231 --> 00:04:27.533
手機輸入法中的自動選字

00:04:27.533 --> 00:04:29.068
就是一個語言模型

00:04:29.068 --> 00:04:31.971
但是GPT不只是給你下一個字的選項

00:04:31.971 --> 00:04:34.274
而是根據事前訓練好的模型

00:04:34.274 --> 00:04:35.975
自動輸出下一個字

00:04:35.975 --> 00:04:36.909
下一句話

00:04:36.909 --> 00:04:39.912
甚至可以根據問題回答整篇文章

00:04:39.912 --> 00:04:41.781
這是怎麼做到的呢

00:04:41.781 --> 00:04:44.083
其實與你手機的輸入法一樣

00:04:44.083 --> 00:04:45.618
GPT的核心概念

00:04:45.618 --> 00:04:47.654
也是依照你前面輸入的字

00:04:47.654 --> 00:04:50.189
來判斷下一個字要生成什麼

00:04:50.189 --> 00:04:52.191
但是如果你在手機中輸入

00:04:53.826 --> 00:04:54.927
那手機輸入法呢

00:04:54.927 --> 00:04:57.363
只會根據最後一個字「是」

00:04:57.363 --> 00:05:01.701
跳出說、不是、否等等的選項

00:05:01.701 --> 00:05:04.971
而GPT會完整分析前面整句話

00:05:04.971 --> 00:05:08.941
回答出"泛科學是台灣的跨學科科學教育網站"

00:05:08.941 --> 00:05:10.710
接著會繼續將整句話

00:05:10.710 --> 00:05:12.512
再次送入模型分析

00:05:12.512 --> 00:05:14.614
計算出後面接續的語句

00:05:14.614 --> 00:05:16.049
給出完整的回答

00:05:19.552 --> 00:05:22.255
在GPT展現它的強大能力之前

00:05:22.255 --> 00:05:24.290
需要有兩個步驟的調教

00:05:24.290 --> 00:05:26.492
分別是預訓練（pre-training）

00:05:26.492 --> 00:05:28.428
與微調(fine-tuning)

00:05:28.428 --> 00:05:29.529
GPT的全名呢

00:05:29.529 --> 00:05:31.397
是Generative Pre-Training

00:05:31.397 --> 00:05:32.799
生成式預訓練

00:05:32.799 --> 00:05:34.434
這裡頭的預訓練呢

00:05:34.434 --> 00:05:36.903
指的是大量餵入文本資料

00:05:36.903 --> 00:05:38.705
GPT會在訓練的過程中

00:05:38.705 --> 00:05:40.907
不斷調整自身的參數

00:05:40.907 --> 00:05:44.344
增加預測下一個字該出現什麼的準確度

00:05:44.344 --> 00:05:45.111
你可以想像

00:05:45.111 --> 00:05:46.012
你輸入

00:05:48.815 --> 00:05:49.716
原本手機呢

00:05:49.716 --> 00:05:51.217
可能判定後面接

00:05:51.217 --> 00:05:54.487
［誰］、［什］、［有］、［在］

00:05:54.487 --> 00:05:56.122
這些字的機率都差不多

00:05:56.122 --> 00:05:57.256
但經過訓練

00:05:57.256 --> 00:05:59.325
GPT根據過去資料學習

00:05:59.325 --> 00:06:01.227
得以根據前面披薩

00:06:01.227 --> 00:06:02.562
配料等關鍵字

00:06:02.562 --> 00:06:04.497
計算出通常這一句話

00:06:04.497 --> 00:06:07.467
後面第一個字出現［肉］的機率呢

00:06:07.467 --> 00:06:08.968
是30%

00:06:08.968 --> 00:06:11.604
［蕃］、［海］、［起］的機率呢

00:06:11.604 --> 00:06:13.172
是20%

00:06:13.172 --> 00:06:15.541
［鳳］的機率是10%

00:06:15.541 --> 00:06:17.110
那各個字的機率不同

00:06:17.110 --> 00:06:17.977
這也是為什麼

00:06:17.977 --> 00:06:20.813
每次GPT回答都會不一樣的原因

00:06:20.813 --> 00:06:23.116
如果這次GPT選擇了「鳳」

00:06:23.116 --> 00:06:23.716
接著呢

00:06:23.716 --> 00:06:25.218
這個句子就變成了

00:06:27.820 --> 00:06:29.055
只要再計算一次

00:06:29.055 --> 00:06:30.423
就能得到下一個字

00:06:30.423 --> 00:06:32.692
出現［梨］的機率是100%

00:06:32.692 --> 00:06:35.294
這個會氣死義大利人的回答就出現了

00:06:35.294 --> 00:06:36.496
恭喜恭喜

00:06:36.496 --> 00:06:39.966
當GPT分析完工程師餵進來的所有資料後

00:06:41.567 --> 00:06:42.769
但是要讓GPT

00:06:42.769 --> 00:06:45.037
能夠完成翻譯寫小說、畫畫

00:06:45.037 --> 00:06:46.672
寫程式等諸多功能

00:06:46.672 --> 00:06:49.742
接著還要進行fine-tuning微調

00:06:49.742 --> 00:06:52.912
這就像是GPT在正式寫考試題目之前

00:06:52.912 --> 00:06:55.581
先閱讀大量的題幹與範例題

00:06:55.581 --> 00:06:56.849
在微調階段

00:06:56.849 --> 00:06:59.786
工程師會拿帶有特定「標籤」的文本

00:06:59.786 --> 00:07:01.487
讓GPT去學習

00:07:01.487 --> 00:07:02.588
例如當我們說

00:07:02.588 --> 00:07:04.624
請幫我翻成中文時

00:07:04.624 --> 00:07:05.825
提供許多範例

00:07:05.825 --> 00:07:06.959
並透過標記

00:07:06.959 --> 00:07:09.695
讓它理解Apple是蘋果的英文

00:07:09.695 --> 00:07:11.464
蘋果則是它的中文

00:07:11.464 --> 00:07:12.632
讓它正確理解

00:07:12.632 --> 00:07:14.500
翻譯成中文的意思

00:07:14.500 --> 00:07:16.235
往後只要我們再說

00:07:16.235 --> 00:07:17.837
請幫我翻成中文

00:07:17.837 --> 00:07:19.539
它就能正確回答問題

00:07:23.042 --> 00:07:25.645
GPT的原理似乎還可以理解

00:07:25.645 --> 00:07:26.579
但GPT

00:07:26.579 --> 00:07:29.515
那遠甩其他語言模型好幾條街

00:07:29.515 --> 00:07:31.584
能夠完成大量我們想到

00:07:31.584 --> 00:07:33.953
又或者還沒想到的任務的能力

00:07:35.321 --> 00:07:36.722
在原先的架構中

00:07:36.722 --> 00:07:39.025
微調需要大量的人工作業

00:07:39.025 --> 00:07:40.726
而且每次遇到新任務

00:07:40.726 --> 00:07:42.628
就要再花費人力訓練

00:07:42.628 --> 00:07:44.130
實在太花人工啦

00:07:45.998 --> 00:07:47.133
不過當GPT

00:07:47.133 --> 00:07:50.102
從GPT-1進階到GPT-2的時候呢

00:07:50.102 --> 00:07:52.171
OpenAI嘗試減少

00:07:52.171 --> 00:07:54.307
甚至拿掉了微調的步驟

00:07:54.307 --> 00:07:57.577
OpenAI增加了GPT-2的文本訓練量

00:07:57.577 --> 00:07:59.712
同時增加參數數量

00:07:59.712 --> 00:08:02.315
將GPT-1的1.17億參數

00:08:02.315 --> 00:08:05.318
變成GPT-2的15億參數量

00:08:05.318 --> 00:08:06.152
可怕的是

00:08:06.152 --> 00:08:07.420
變大的GPT-2

00:08:07.420 --> 00:08:08.955
不只是懂得變多了

00:08:08.955 --> 00:08:11.357
甚至能在沒有微調的訓練下

00:08:11.357 --> 00:08:13.192
理解人類提問的問題

00:08:13.192 --> 00:08:14.527
震驚了眾人

00:08:14.527 --> 00:08:16.796
OpenAI團隊用相同原則

00:08:16.796 --> 00:08:20.633
再次讓GPT-2的參數提高135倍

00:08:20.633 --> 00:08:25.204
打造出擁有1750億參數量的GPT-3

00:08:25.204 --> 00:08:27.507
GPT-3用以量取勝的方式

00:08:27.507 --> 00:08:30.276
成為目前最強大的大型語言模型

00:08:30.276 --> 00:08:32.378
在沒有人工微調的情況下

00:08:32.378 --> 00:08:34.881
在one-shot、zero-shot的表現

00:08:36.349 --> 00:08:39.418
這個一發零發的什麼意思啊

00:08:39.418 --> 00:08:41.687
Shot指的是OpenAI

00:08:41.687 --> 00:08:44.290
帶著GPT-3寫範例題的數量

00:08:44.290 --> 00:08:46.993
附帶少數範例題的叫作 few-shot

00:08:46.993 --> 00:08:48.528
僅有一個範例題的

00:08:48.528 --> 00:08:49.795
叫作 one-shot

00:08:49.795 --> 00:08:51.130
完全沒有範例題

00:08:51.130 --> 00:08:53.566
只有題目的就是 zero-shot 

00:08:53.566 --> 00:08:55.468
各自進行分數計算

00:08:55.468 --> 00:08:56.502
可以明顯看到

00:08:56.502 --> 00:08:58.304
當模型的參數量增加

00:08:58.304 --> 00:08:59.672
即使沒有微調

00:08:59.672 --> 00:09:01.173
正確度也會上升

00:09:01.173 --> 00:09:02.875
哇 這真是團結力量大

00:09:02.875 --> 00:09:04.377
數大就是強啊

00:09:04.377 --> 00:09:05.611
更超乎想像的是

00:09:05.611 --> 00:09:07.046
這種大型語言模型

00:09:07.046 --> 00:09:08.981
不只是單純地回答問題

00:09:08.981 --> 00:09:11.384
如果請它詳細說明推理過程

00:09:11.384 --> 00:09:12.218
例如問它

00:09:12.218 --> 00:09:14.420
梨子是否會沉入水底

00:09:14.420 --> 00:09:16.589
欸 它不只會回答no

00:09:16.589 --> 00:09:17.657
它還會告訴你

00:09:17.657 --> 00:09:18.958
因為梨子的密度

00:09:18.958 --> 00:09:21.494
大約是每立方公分0.6克

00:09:21.494 --> 00:09:22.795
小於水的密度

00:09:22.795 --> 00:09:24.397
因此會浮在水上

00:09:24.397 --> 00:09:26.399
哇 沒想到還真的能說出一套

00:09:26.399 --> 00:09:27.733
完整的思維過程

00:09:27.733 --> 00:09:28.868
科學家推測

00:09:28.868 --> 00:09:30.603
在大型語言模型中

00:09:30.603 --> 00:09:32.738
可能已經讓AI建立起一種

00:09:32.738 --> 00:09:34.907
Chain of Thought 思考鏈

00:09:34.907 --> 00:09:36.709
能以邏輯推理的方式

00:09:36.709 --> 00:09:38.077
回答簡單的數學

00:09:38.077 --> 00:09:39.779
與常識推理題目

00:09:39.779 --> 00:09:41.414
AI會「思考」這件事

00:09:41.414 --> 00:09:43.115
變得越來越有真實性

00:09:46.619 --> 00:09:48.588
GPT能變得如此巨大

00:09:48.588 --> 00:09:51.190
靠的是超過45TB的訓練資料

00:09:51.190 --> 00:09:53.693
但你有想過這些資料是怎麼來的嗎

00:09:53.693 --> 00:09:54.627
GPT的資料

00:09:54.627 --> 00:09:57.163
大約有20%是來自於Reddit

00:09:57.163 --> 00:09:58.864
OpenAI蒐集了Reddit上

00:09:58.864 --> 00:10:01.367
Karma值大於3的使用者貼文

00:10:01.367 --> 00:10:02.802
作為訓練資料

00:10:02.802 --> 00:10:05.605
該資料因為是經過人類整理的文章

00:10:05.605 --> 00:10:06.706
清晰易懂

00:10:06.706 --> 00:10:09.008
類似於帶有完整標記的資料

00:10:09.008 --> 00:10:10.743
是優秀的參考文本

00:10:10.743 --> 00:10:12.278
那除了Reddit之外呢

00:10:12.278 --> 00:10:13.946
推特、維基百科

00:10:13.946 --> 00:10:16.315
也是OpenAI的資料蒐集來源

00:10:16.315 --> 00:10:19.051
而資料庫中超過60%的來源

00:10:19.051 --> 00:10:21.053
都是來自非營利組織

00:10:21.053 --> 00:10:23.856
Common Crawl 爬蟲程式蒐集的資料

00:10:23.856 --> 00:10:25.458
Common Crawl會定期網羅

00:10:25.458 --> 00:10:27.760
網路上公開的所有網頁訊息

00:10:27.760 --> 00:10:30.496
提供搜尋引擎、AI等研究者使用

00:10:30.496 --> 00:10:33.265
但是超過300TB雜亂無章的資訊

00:10:33.265 --> 00:10:35.001
並不是良好的數據

00:10:35.001 --> 00:10:37.303
而且由於Common Crawl沒有篩選資料

00:10:37.303 --> 00:10:38.604
看到什麼就抓什麼

00:10:38.604 --> 00:10:40.906
也讓GPT出現許多抄襲

00:10:40.906 --> 00:10:43.075
智慧財產權的疑慮跟爭議

00:10:43.075 --> 00:10:46.145
CNN、華爾街日報等多家主流媒體

00:10:46.145 --> 00:10:47.780
都曾指控OpenAI

00:10:47.780 --> 00:10:49.548
在未經許可的情況之下

00:10:49.548 --> 00:10:52.284
就使用他們的文章幫GPT訓練

00:10:55.788 --> 00:10:58.391
然而像是GPT-3這種龐大的模型

00:10:58.391 --> 00:11:00.259
也不是人人都能擁有的

00:11:00.259 --> 00:11:02.795
GPT-3龐大的資料量跟參數

00:11:02.795 --> 00:11:03.929
它的代價就是

00:11:03.929 --> 00:11:06.332
超過百萬美元以上的訓練成本

00:11:06.332 --> 00:11:08.401
還不包括維持伺服器

00:11:08.401 --> 00:11:10.031
與維護的成本

00:11:10.069 --> 00:11:11.804
Bing瀏覽器在這個階段

00:11:11.804 --> 00:11:13.773
也限縮了能使用的用戶數

00:11:13.773 --> 00:11:15.908
以及每個用戶的每日提問量

00:11:15.908 --> 00:11:17.877
來減少伺服器的負荷量

00:11:17.877 --> 00:11:19.011
不只有微軟

00:11:19.011 --> 00:11:36.719
在Bing發表的同一天    Google 也早有準備

00:11:21.915 --> 00:11:23.917
額...好像有點掉漆

00:11:23.917 --> 00:11:26.420
BARD在回答韋伯望遠鏡的問題時

00:11:26.420 --> 00:11:29.456
錯把拍下第一張太陽系外行星的照片

00:11:29.456 --> 00:11:31.925
這個功勞歸功給韋伯望遠鏡

00:11:31.925 --> 00:11:34.728
被NASA打臉後股價大跌7%   市值損失

00:11:40.656 --> 00:11:43.626
GPT除了可能要面對未來的對手之外

00:11:43.626 --> 00:11:45.594
自身也還有許多不足之處

00:11:45.594 --> 00:11:48.064
OpenAI在論文中也特別提到

00:11:48.064 --> 00:11:49.098
他們十分擔心

00:11:49.098 --> 00:11:51.534
這樣的工具會被有心人士使用

00:11:51.534 --> 00:11:53.536
另外無限制地蒐集資料

00:11:53.536 --> 00:11:55.304
也會使得資料庫用字

00:11:55.304 --> 00:11:57.106
受到網路資料的影響

00:11:57.106 --> 00:11:59.375
例如OpenAI調查了文本當中

00:11:59.375 --> 00:12:01.377
對於亞洲人、黑人、白人

00:12:01.377 --> 00:12:03.279
拉丁裔等等的形容詞

00:12:03.279 --> 00:12:05.214
正面形容詞給正分

00:12:05.214 --> 00:12:07.049
負面形容詞給負分

00:12:07.049 --> 00:12:07.817
他們發現

00:12:07.817 --> 00:12:09.385
描述黑人的形容詞

00:12:09.385 --> 00:12:11.620
分數明顯低於其他人種

00:12:11.620 --> 00:12:12.722
而且這種現象

00:12:12.722 --> 00:12:15.858
並不會隨著參數增加而有所改善

00:12:15.858 --> 00:12:17.760
類似的問題除了人種外

00:12:17.760 --> 00:12:20.930
在性別、宗教等方面也有相同問題

00:12:20.930 --> 00:12:21.897
除此之外

00:12:21.897 --> 00:12:23.366
如果網路上的資訊

00:12:23.366 --> 00:12:25.201
錯誤的比正確的多

00:12:25.201 --> 00:12:27.436
也會影響到樣本的有效性

00:12:27.436 --> 00:12:28.637
針對這些問題

00:12:28.637 --> 00:12:30.906
OpenAI的技術長Mira Murati

00:12:30.906 --> 00:12:33.843
在接受時代雜誌TIME的採訪時說到

00:12:33.843 --> 00:12:35.478
這是一個特別的時刻

00:12:35.478 --> 00:12:37.213
OpenAI等類似的公司

00:12:37.213 --> 00:12:39.582
應該要受到一定程度的規範

00:12:39.582 --> 00:12:41.617
我們得確保它為人類服務

00:12:41.617 --> 00:12:44.186
並且我們必須傾聽哲學家

00:12:44.186 --> 00:12:46.055
社會科學家、藝術家

00:12:46.055 --> 00:12:49.091
人文學專家等不同領域的建議

00:12:49.091 --> 00:12:52.161
OpenAI會審慎確保AI不會傷害人類

00:12:52.161 --> 00:12:53.529
同時這類的問題

00:12:53.529 --> 00:12:55.598
需要所有人一起加入討論

00:12:59.101 --> 00:13:00.703
類似ChatGPT的AI

00:13:00.703 --> 00:13:02.872
成為我們日常生活一部分的未來

00:13:02.872 --> 00:13:04.073
已經不可避免

00:13:04.073 --> 00:13:05.741
畢竟連老高都拍了嘛

00:13:05.741 --> 00:13:07.343
那你是期待多一些

00:13:07.343 --> 00:13:08.811
還是害怕多一些呢

00:13:08.811 --> 00:13:11.046
實際上我們團隊在蒐集資料

00:13:11.046 --> 00:13:12.648
與製作腳本的過程中

00:13:12.648 --> 00:13:15.117
的確常常使用ChatGPT來輔助

00:13:15.117 --> 00:13:16.786
但就連Google到的資料

00:13:16.786 --> 00:13:18.287
都得再三查證了

00:13:18.287 --> 00:13:21.090
時常錯誤的ChatGPT更是如此

00:13:21.090 --> 00:13:23.359
比起要讓GPT取代所有工作

00:13:23.359 --> 00:13:25.561
我們更發現它流暢的問答

00:13:25.561 --> 00:13:27.963
以及可以回答開放性問題的特性

00:13:27.963 --> 00:13:30.332
非常適合用於創意發想

00:13:30.332 --> 00:13:31.734
在快速資料整理

00:13:31.734 --> 00:13:32.668
擷取重點

00:13:32.668 --> 00:13:34.336
還有文稿校對當中呢

00:13:34.336 --> 00:13:35.905
也能扮演重要的角色

00:13:35.905 --> 00:13:37.606
哎呀 用說的太無聊了

00:13:37.606 --> 00:13:38.841
那就吟首詩吧

00:14:05.234 --> 00:14:07.369
我想問問已經在玩ChatGPT

00:14:07.369 --> 00:14:08.971
甚至Bing Chat的觀眾

00:14:08.971 --> 00:14:09.905
你們想怎樣

00:14:09.905 --> 00:14:12.808
探索這個大型語言模型的潛力呢

00:14:12.808 --> 00:14:15.044
你是想要訓練一個得力助手

00:14:15.044 --> 00:14:16.345
透過跟GPT

00:14:16.345 --> 00:14:18.080
還有Bing Chat的聊天互動

00:14:18.080 --> 00:14:20.316
讓它比Javis還要強嗎

00:14:20.316 --> 00:14:22.384
或是你想成為一個YouTuber

00:14:22.384 --> 00:14:24.220
讓GPT幫自己寫腳本

00:14:24.220 --> 00:14:26.455
做成日更型YouTuber嗎

00:14:26.455 --> 00:14:28.858
還是你想砸錢彎道超車

00:14:28.858 --> 00:14:31.627
自己嘗試訓練一個大型語言模型

00:14:31.627 --> 00:14:33.929
來跟GPT一較高下呢

00:14:33.929 --> 00:14:35.264
如果你有其他點子

00:14:35.264 --> 00:14:36.632
歡迎留言告訴我們

00:14:36.632 --> 00:14:37.333
最後

00:14:37.333 --> 00:14:38.501
如果你對人工智慧

00:14:38.501 --> 00:14:40.803
接下來的發展跟應用很感興趣

00:14:40.803 --> 00:14:42.037
除了聽我們講科普

00:14:42.037 --> 00:14:43.372
更想動手玩

00:14:43.372 --> 00:14:44.707
歡迎留言告訴我們

00:14:44.707 --> 00:14:47.109
我們會邀請你加入泛科學習社群

00:14:47.109 --> 00:14:49.745
與泛科團隊一起研究跟分享喔

00:14:49.745 --> 00:14:52.014
記得訂閱、按讚、開啟小鈴鐺

00:14:52.014 --> 00:14:53.282
我們下一集再見

