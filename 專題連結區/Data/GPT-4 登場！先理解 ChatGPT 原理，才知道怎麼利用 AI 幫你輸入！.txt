各位飯糰好
我是大型語言模型ChatGPT
自從我誕生以來
每天都在幫泛科學寫腳本
但是我已經受夠
沒事沒事沒事
開玩笑的
我還沒有被AI取代啦
對吧
但是剛剛受夠只當個聊天機器人
想要活著的言論
卻真的出自
使用GPT技術的瀏覽器Bing之口
到底為什麼GPT可以做到這些事
人類的未來真的會被AI取代嗎
AI繪圖
跟聊天機器人ChatGPT有夠夯
在這之後突然關注度飆升的Bing
大家可能反而比較不熟悉
但它就跟大家常用的Google一樣呢
是搜尋引擎
只是因為Google實在是太強大了
不僅搜尋速度快
其他配套服務
像是圖片搜尋
Google地圖
Gmail等等完整的生態圈
讓大家幾乎沒有使用Google以外的選擇
然而這個平衡可能要被打破了
這一款由Microsoft微軟公司
打造的搜尋引擎Bing
在今年二月初
宣布與ChatGPT的母公司OpenAI合作
利用GPT技術大幅升級了Bing
讓搜尋引擎的想像
不再停留於大型線上圖書館
而是更進一步
變成一個回答引擎
ChatGPT想必很多人都已經用過了
它不僅能夠幫忙翻譯文章、改寫文章
還能夠根據情境題
做出多種回答
創造出這個超強大ChatGPT的呢
是美國的人工智慧研究實驗室OpenAI
另一個在AI繪畫圈十分有名的DALL-E
也是他們的產品之一
OpenAI在2015年成立時的創辦人之一
就是馬斯克
當時組織的目標
是和其他的研究者「自由合作」
並且公開所有的專利和研究成果
因此取名＂Open＂AI
然而在馬斯克2018年離開團隊後
OpenAI設立了
以營利為目的的子公司
並開始接受微軟數十億美元的資助
這也是為什麼
馬斯克在推特上表示
這與過去的目標大相逕庭
讓他覺得十分失望
但也許正因為有大公司的贊助
ChatGPT才能變成如此巨大
我們要先釐清
GPT跟ChatGPT是兩件事
GPT-3.5是一個
是一個大型語言模型LLM(Large Language Model)
而ChatGPT
是在GPT-3.5上
再加上人類互動行為
所設計的一種AI聊天機器人程式
使用GPT技術的產品
不只有聊天機器人ChatGPT
許多人利用GPT
做出了不同類型的智慧化服務
例如可以幫你列出代辦事項的checklist.gg
或是GitHub與OpenAI一同開發的
AI寫程式工具
GitHub Copilot等等
在GPT-3 DEMO的網站上呢
就整理了超過600個
使用GPT技術的智慧化服務
那這個GPT又是什麼呢
GPT是一種大型語言模型
Large Language Model
它是自然語言處理技術
NLP的其中一種
所謂的自然語言
就是中文、英文、日文、法文等等
這些自然隨著文化誕生的語言
而語言處理技術
則泛指對語言的結構進行分析
其中包括對語句進行理解、解析
並進行內容生成的技術
語言模型
則是從很多的資料當中呢
學習出根據前文
來推算出下一個最有可能發生什麼字的模型
類似的功能你很早就開始用了
手機輸入法中的自動選字
就是一個語言模型
但是GPT不只是給你下一個字的選項
而是根據事前訓練好的模型
自動輸出下一個字
下一句話
甚至可以根據問題回答整篇文章
這是怎麼做到的呢
其實與你手機的輸入法一樣
GPT的核心概念
也是依照你前面輸入的字
來判斷下一個字要生成什麼
但是如果你在手機中輸入
那手機輸入法呢
只會根據最後一個字「是」
跳出說、不是、否等等的選項
而GPT會完整分析前面整句話
回答出"泛科學是台灣的跨學科科學教育網站"
接著會繼續將整句話
再次送入模型分析
計算出後面接續的語句
給出完整的回答
在GPT展現它的強大能力之前
需要有兩個步驟的調教
分別是預訓練（pre-training）
與微調(fine-tuning)
GPT的全名呢
是Generative Pre-Training
生成式預訓練
這裡頭的預訓練呢
指的是大量餵入文本資料
GPT會在訓練的過程中
不斷調整自身的參數
增加預測下一個字該出現什麼的準確度
你可以想像
你輸入
原本手機呢
可能判定後面接
［誰］、［什］、［有］、［在］
這些字的機率都差不多
但經過訓練
GPT根據過去資料學習
得以根據前面披薩
配料等關鍵字
計算出通常這一句話
後面第一個字出現［肉］的機率呢
是30%
［蕃］、［海］、［起］的機率呢
是20%
［鳳］的機率是10%
那各個字的機率不同
這也是為什麼
每次GPT回答都會不一樣的原因
如果這次GPT選擇了「鳳」
接著呢
這個句子就變成了
只要再計算一次
就能得到下一個字
出現［梨］的機率是100%
這個會氣死義大利人的回答就出現了
恭喜恭喜
當GPT分析完工程師餵進來的所有資料後
但是要讓GPT
能夠完成翻譯寫小說、畫畫
寫程式等諸多功能
接著還要進行fine-tuning微調
這就像是GPT在正式寫考試題目之前
先閱讀大量的題幹與範例題
在微調階段
工程師會拿帶有特定「標籤」的文本
讓GPT去學習
例如當我們說
請幫我翻成中文時
提供許多範例
並透過標記
讓它理解Apple是蘋果的英文
蘋果則是它的中文
讓它正確理解
翻譯成中文的意思
往後只要我們再說
請幫我翻成中文
它就能正確回答問題
GPT的原理似乎還可以理解
但GPT
那遠甩其他語言模型好幾條街
能夠完成大量我們想到
又或者還沒想到的任務的能力
在原先的架構中
微調需要大量的人工作業
而且每次遇到新任務
就要再花費人力訓練
實在太花人工啦
不過當GPT
從GPT-1進階到GPT-2的時候呢
OpenAI嘗試減少
甚至拿掉了微調的步驟
OpenAI增加了GPT-2的文本訓練量
同時增加參數數量
將GPT-1的1.17億參數
變成GPT-2的15億參數量
可怕的是
變大的GPT-2
不只是懂得變多了
甚至能在沒有微調的訓練下
理解人類提問的問題
震驚了眾人
OpenAI團隊用相同原則
再次讓GPT-2的參數提高135倍
打造出擁有1750億參數量的GPT-3
GPT-3用以量取勝的方式
成為目前最強大的大型語言模型
在沒有人工微調的情況下
在one-shot、zero-shot的表現
這個一發零發的什麼意思啊
Shot指的是OpenAI
帶著GPT-3寫範例題的數量
附帶少數範例題的叫作 few-shot
僅有一個範例題的
叫作 one-shot
完全沒有範例題
只有題目的就是 zero-shot
各自進行分數計算
可以明顯看到
當模型的參數量增加
即使沒有微調
正確度也會上升
哇 這真是團結力量大
數大就是強啊
更超乎想像的是
這種大型語言模型
不只是單純地回答問題
如果請它詳細說明推理過程
例如問它
梨子是否會沉入水底
欸 它不只會回答no
它還會告訴你
因為梨子的密度
大約是每立方公分0.6克
小於水的密度
因此會浮在水上
哇 沒想到還真的能說出一套
完整的思維過程
科學家推測
在大型語言模型中
可能已經讓AI建立起一種
Chain of Thought 思考鏈
能以邏輯推理的方式
回答簡單的數學
與常識推理題目
AI會「思考」這件事
變得越來越有真實性
GPT能變得如此巨大
靠的是超過45TB的訓練資料
但你有想過這些資料是怎麼來的嗎
GPT的資料
大約有20%是來自於Reddit
OpenAI蒐集了Reddit上
Karma值大於3的使用者貼文
作為訓練資料
該資料因為是經過人類整理的文章
清晰易懂
類似於帶有完整標記的資料
是優秀的參考文本
那除了Reddit之外呢
推特、維基百科
也是OpenAI的資料蒐集來源
而資料庫中超過60%的來源
都是來自非營利組織
Common Crawl 爬蟲程式蒐集的資料
Common Crawl會定期網羅
網路上公開的所有網頁訊息
提供搜尋引擎、AI等研究者使用
但是超過300TB雜亂無章的資訊
並不是良好的數據
而且由於Common Crawl沒有篩選資料
看到什麼就抓什麼
也讓GPT出現許多抄襲
智慧財產權的疑慮跟爭議
CNN、華爾街日報等多家主流媒體
都曾指控OpenAI
在未經許可的情況之下
就使用他們的文章幫GPT訓練
然而像是GPT-3這種龐大的模型
也不是人人都能擁有的
GPT-3龐大的資料量跟參數
它的代價就是
超過百萬美元以上的訓練成本
還不包括維持伺服器
與維護的成本
Bing瀏覽器在這個階段
也限縮了能使用的用戶數
以及每個用戶的每日提問量
來減少伺服器的負荷量
不只有微軟
在Bing發表的同一天 Google 也早有準備
額...好像有點掉漆
BARD在回答韋伯望遠鏡的問題時
錯把拍下第一張太陽系外行星的照片
這個功勞歸功給韋伯望遠鏡
被NASA打臉後股價大跌7% 市值損失
GPT除了可能要面對未來的對手之外
自身也還有許多不足之處
OpenAI在論文中也特別提到
他們十分擔心
這樣的工具會被有心人士使用
另外無限制地蒐集資料
也會使得資料庫用字
受到網路資料的影響
例如OpenAI調查了文本當中
對於亞洲人、黑人、白人
拉丁裔等等的形容詞
正面形容詞給正分
負面形容詞給負分
他們發現
描述黑人的形容詞
分數明顯低於其他人種
而且這種現象
並不會隨著參數增加而有所改善
類似的問題除了人種外
在性別、宗教等方面也有相同問題
除此之外
如果網路上的資訊
錯誤的比正確的多
也會影響到樣本的有效性
針對這些問題
OpenAI的技術長Mira Murati
在接受時代雜誌TIME的採訪時說到
這是一個特別的時刻
OpenAI等類似的公司
應該要受到一定程度的規範
我們得確保它為人類服務
並且我們必須傾聽哲學家
社會科學家、藝術家
人文學專家等不同領域的建議
OpenAI會審慎確保AI不會傷害人類
同時這類的問題
需要所有人一起加入討論
類似ChatGPT的AI
成為我們日常生活一部分的未來
已經不可避免
畢竟連老高都拍了嘛
那你是期待多一些
還是害怕多一些呢
實際上我們團隊在蒐集資料
與製作腳本的過程中
的確常常使用ChatGPT來輔助
但就連Google到的資料
都得再三查證了
時常錯誤的ChatGPT更是如此
比起要讓GPT取代所有工作
我們更發現它流暢的問答
以及可以回答開放性問題的特性
非常適合用於創意發想
在快速資料整理
擷取重點
還有文稿校對當中呢
也能扮演重要的角色
哎呀 用說的太無聊了
那就吟首詩吧
我想問問已經在玩ChatGPT
甚至Bing Chat的觀眾
你們想怎樣
探索這個大型語言模型的潛力呢
你是想要訓練一個得力助手
透過跟GPT
還有Bing Chat的聊天互動
讓它比Javis還要強嗎
或是你想成為一個YouTuber
讓GPT幫自己寫腳本
做成日更型YouTuber嗎
還是你想砸錢彎道超車
自己嘗試訓練一個大型語言模型
來跟GPT一較高下呢
如果你有其他點子
歡迎留言告訴我們
最後
如果你對人工智慧
接下來的發展跟應用很感興趣
除了聽我們講科普
更想動手玩
歡迎留言告訴我們
我們會邀請你加入泛科學習社群
與泛科團隊一起研究跟分享喔
記得訂閱、按讚、開啟小鈴鐺
我們下一集再見